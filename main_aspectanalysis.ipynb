{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of this project is to complete the Aspect-based Sentiment Analysis (ABSA) task. ABSA task aims at identifying the sentiment polarity (e.g.positive, negative, neutral) of one specific aspect in its context sentence usig Deep Learning Models, Natural language Processing and Attention Mechanism. \n",
    "\n",
    "### We  design different model variants to differently integrate the aspect information in our model structures (e.g. different locations, different integration methods etc. ) using Attention Mechanism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77oROC6IeI7T"
   },
   "source": [
    "# 1.Dataset Processing\n",
    "\n",
    "**Note:**\n",
    "\n",
    "*   Change path to dataset location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4hshFTJxH4f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSm-tJRx5VWQ",
    "outputId": "d22803bc-58e8-4c8d-fae2-c6abe0804b13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hn8xs1D9xLH4"
   },
   "outputs": [],
   "source": [
    "#Loading Train set\n",
    "train_df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP/Assignment/train.json', lines=True)\n",
    "train_data = train_df['data'][0]\n",
    "Sentence, Aspect, Polarity = zip(*train_data)\n",
    "train_data = pd.DataFrame()\n",
    "train_data[\"Sentences\"]= Sentence\n",
    "train_data[\"Aspect\"]= Aspect\n",
    "train_data[\"Polarity\"]= Polarity\n",
    "corpus_np = train_data.to_numpy() # Creating a numpy array for train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCIUt0NBka0U"
   },
   "outputs": [],
   "source": [
    "#Loading Test set\n",
    "test_df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP/Assignment/test.json', lines=True)\n",
    "test_data = test_df['data'][0]\n",
    "Sentence, Aspect, Polarity = zip(*test_data)\n",
    "test_data = pd.DataFrame()\n",
    "test_data[\"Sentences\"]= Sentence\n",
    "test_data[\"Aspect\"]= Aspect\n",
    "test_data[\"Polarity\"]= Polarity\n",
    "test_corpus_np = test_data.to_numpy() # Creating a numpy array for test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxISOyfwkvr3"
   },
   "outputs": [],
   "source": [
    "#Loading Validation set\n",
    "val_df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP/Assignment/val.json', lines=True)\n",
    "val_data = val_df['data'][0]\n",
    "Sentence, Aspect, Polarity = zip(*val_data)\n",
    "val_data = pd.DataFrame()\n",
    "val_data[\"Sentences\"]= Sentence\n",
    "val_data[\"Aspect\"]= Aspect\n",
    "val_data[\"Polarity\"]= Polarity\n",
    "val_corpus_np = val_data.to_numpy()# Creating a numpy array for val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "fmVg8joDbGOL",
    "outputId": "26ea6760-38eb-426d-de9b-e5c13bce3106"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"train_data\",\n  \"rows\": 7090,\n  \"fields\": [\n    {\n      \"column\": \"Sentences\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3149,\n        \"samples\": [\n          \"Our waiter took his time taking our drink orders ('97 Italian red was good).\",\n          \"The waiter was able to answer any questions I had about the wine or food.\",\n          \"the heroes are enough for lunch and dinner.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Aspect\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"place\",\n          \"price\",\n          \"food\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Polarity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"neutral\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "train_data"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-310456e7-d867-45a0-b23d-a3ec15b285fd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It might be the best sit down food I've had in...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It might be the best sit down food I've had in...</td>\n",
       "      <td>place</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hostess was extremely accommodating when we ar...</td>\n",
       "      <td>staff</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hostess was extremely accommodating when we ar...</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We were a couple of minutes late for our reser...</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-310456e7-d867-45a0-b23d-a3ec15b285fd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-310456e7-d867-45a0-b23d-a3ec15b285fd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-310456e7-d867-45a0-b23d-a3ec15b285fd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-d34293f1-cddc-4831-952c-a8446cd6fd23\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d34293f1-cddc-4831-952c-a8446cd6fd23')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-d34293f1-cddc-4831-952c-a8446cd6fd23 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                           Sentences         Aspect  Polarity\n",
       "0  It might be the best sit down food I've had in...           food  positive\n",
       "1  It might be the best sit down food I've had in...          place   neutral\n",
       "2  Hostess was extremely accommodating when we ar...          staff  positive\n",
       "3  Hostess was extremely accommodating when we ar...  miscellaneous   neutral\n",
       "4  We were a couple of minutes late for our reser...  miscellaneous   neutral"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "JOY_dgEabSjM",
    "outputId": "e30895fa-134c-4a45-e1fe-cd4ca9df97c2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "train_data.groupby('Polarity').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "7jz_ZrbPbOS1",
    "outputId": "855e5c4c-20a2-45a3-c3fb-50db4ee3c3d7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.groupby('Aspect').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDJdif1igMES"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aA0WWnO8jaY",
    "outputId": "8d05674e-8f23-4fb8-fa1e-ad42480807e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!python -m nltk.downloader punkt\n",
    "!python -m nltk.downloader stopwords\n",
    "!python -m nltk.downloader wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pQKoty-30Oc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rxahzXabUJJ"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "  sww = stopwords.words()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  for i in range(len(data)):\n",
    "    data[i][0] = re.sub(r'[^\\w\\s]',' ',data[i][0]) # remove punctuations and replacing with space\n",
    "    data[i][0] = word_tokenize(data[i][0]) # Word Tokenization\n",
    "    data[i][0] = [t.lower() for t in data[i][0]] # changing to lowercase\n",
    "    data[i][0] = [w for w in data[i][0] if not w in sww] # remove stopwords\n",
    "    data[i][0] = [lemmatizer.lemmatize(w) for w in data[i][0]] # Lemmatization\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_ggQL6hcxqb"
   },
   "outputs": [],
   "source": [
    "def preprocess_sww(data):\n",
    "  #sww = stopwords.words()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  for i in range(len(data)):\n",
    "    data[i][0] = re.sub(r'[^\\w\\s]',' ',data[i][0]) # remove punctuations and replacing with space\n",
    "    data[i][0] = word_tokenize(data[i][0]) # Word Tokenization\n",
    "    data[i][0] = [t.lower() for t in data[i][0]] # changing to lowercase\n",
    "    #data[i][0] = [w for w in data[i][0] if not w in sww] # remove stopwords\n",
    "    data[i][0] = [lemmatizer.lemmatize(w) for w in data[i][0]] # Lemmatization\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgVVnk0C9R1l"
   },
   "outputs": [],
   "source": [
    "def preprocess_c_sww(data):\n",
    "    # If custom_stopwords is not provided, use the default stopwords\n",
    "    sww=stopwords.words()\n",
    "    custom_stopwords = ['not', 'no', 'never', 'should','shouldnt', 'would', 'wouldnt', 'wont', 'isnt', 'hadnt', 'again', 'bc',\n",
    "                    'hasnt', 'havent', 'wasnt', 'werent']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Preprocess each data point\n",
    "    for i in range(len(data)):\n",
    "        # Remove punctuations and replace them with space\n",
    "        data[i][0] = re.sub(r'[^\\w\\s]', ' ', data[i][0])\n",
    "        # Tokenize the sentence\n",
    "        data[i][0] = word_tokenize(data[i][0])\n",
    "        # Convert words to lowercase\n",
    "        data[i][0] = [t.lower() for t in data[i][0]]\n",
    "        # Remove stopwords except for custom stopwords\n",
    "        data[i][0] = [w for w in data[i][0] if w in custom_stopwords or w not in stopwords.words()]\n",
    "        # Lemmatization\n",
    "        data[i][0] = [lemmatizer.lemmatize(w) for w in data[i][0]]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqN_i3jhbmit"
   },
   "outputs": [],
   "source": [
    "corpus_np_cp=np.copy(corpus_np)\n",
    "test_corpus_np_cp=np.copy(test_corpus_np)\n",
    "val_corpus_np_cp=np.copy(val_corpus_np)\n",
    "\n",
    "train_corpus = preprocess(corpus_np_cp) # Preprocessing train corpus\n",
    "test_corpus = preprocess(test_corpus_np_cp) # Preprocessing test corpus\n",
    "val_corpus = preprocess(val_corpus_np_cp) # Preprocessing validation corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLXXpkUVc6eQ"
   },
   "outputs": [],
   "source": [
    "corpus_np_cp_1=np.copy(corpus_np)\n",
    "test_corpus_np_1=np.copy(test_corpus_np)\n",
    "val_corpus_np_1=np.copy(val_corpus_np)\n",
    "\n",
    "train_corpus_sww = preprocess_sww(corpus_np_cp_1) # Preprocessing train corpus without removing Stopwords\n",
    "test_corpus_sww = preprocess_sww(test_corpus_np_1) # Preprocessing test corpus without removing Stopwords\n",
    "val_corpus_sww = preprocess_sww(val_corpus_np_1) # Preprocessing validation corpus without removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA8FfDY-Ag3a"
   },
   "outputs": [],
   "source": [
    "train_corpus_c_sww = preprocess_c_sww(corpus_np) # Preprocessing train corpus removing custom Stopwords\n",
    "test_corpus_c_sww = preprocess_c_sww(test_corpus_np) # Preprocessing test corpus removing custom Stopwords\n",
    "val_corpus_c_sww = preprocess_c_sww(val_corpus_np) # Preprocessing validation corpus removing custom Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0yNUbzJgsZH",
    "outputId": "3e64b8bf-ed8b-4605-fc78-78527057ab62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length: 41\n",
      "Position: 4042\n"
     ]
    }
   ],
   "source": [
    "max_length, position = max((len(r[0]), i) for i, r in enumerate(train_corpus_c_sww))\n",
    "\n",
    "print(\"Maximum Length:\", max_length)\n",
    "print(\"Position:\", position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_XIKwfOFslH"
   },
   "source": [
    "### Using Glove-wiki-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-StZW6nlF05u"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-50\") # Using pre-trained word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Vt59roP8jtc"
   },
   "source": [
    "# 2. Model Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTEeuAvV7Mst"
   },
   "source": [
    "## Bi-LSTM Varient-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVtsLLlMcL75"
   },
   "source": [
    "### Batch Creation Varient-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cynd_gBcZseF"
   },
   "outputs": [],
   "source": [
    "def model_1_batches(data, batch_size, max_len):\n",
    "    batches = []\n",
    "    polarity_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = []\n",
    "\n",
    "        for j in range(i, min(i + batch_size, len(data))):\n",
    "            sentence = data[j][0]\n",
    "            aspect = data[j][1]\n",
    "            polarity = data[j][2]\n",
    "            embedding_list = []\n",
    "            for word in sentence: # Check if word exist in pre-trained word embedding else appending zero vector\n",
    "                if str(word) in word_vectors:\n",
    "                    embedding_list.append(word_vectors[word])\n",
    "                else:\n",
    "                    embedding_list.append(np.zeros(50))\n",
    "\n",
    "            embedding_list.append(word_vectors[aspect])\n",
    "\n",
    "            for _ in range(max_len - len(embedding_list)):\n",
    "                embedding_list.append(np.zeros(50))\n",
    "\n",
    "            sentence_vectors = torch.tensor(embedding_list, dtype=torch.float32)\n",
    "            polarity = torch.tensor(polarity_map[polarity], dtype=torch.long)\n",
    "\n",
    "            batch.append((sentence_vectors, polarity))\n",
    "\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJAeK5tocQtc"
   },
   "source": [
    "### Bi-LSTM Model\n",
    "\n",
    "Input format for the model: \"Sentence\" + \"Aspect\" + \"Padding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "j92I3H1AbdWa"
   },
   "outputs": [],
   "source": [
    "# Define the BiLSTM model\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,batch_first =True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_size,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x,(h_n,c_n) = self.lstm(x)\n",
    "\n",
    "        hidden_out = torch.cat((h_n[2,:,:],h_n[3,:,:]),1)\n",
    "        output = self.fc(hidden_out)\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11cHTNKfY3Gw"
   },
   "source": [
    "### Training: without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyozA8oiY0kp",
    "outputId": "96e1f322-bc40-4dc5-c674-5e1d24e5eea5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-8fe9946e8c6e>:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  sentence_vectors = torch.tensor(embedding_list, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.5661\n",
      "Epoch [2/20], Loss: 26.9541\n",
      "Epoch [3/20], Loss: 24.6914\n",
      "Epoch [4/20], Loss: 23.9469\n",
      "Epoch [5/20], Loss: 23.3950\n",
      "Epoch [6/20], Loss: 22.8834\n",
      "Epoch [7/20], Loss: 22.4746\n",
      "Epoch [8/20], Loss: 22.0010\n",
      "Epoch [9/20], Loss: 21.6393\n",
      "Epoch [10/20], Loss: 21.2729\n",
      "Epoch [11/20], Loss: 20.9831\n",
      "Epoch [12/20], Loss: 20.5057\n",
      "Epoch [13/20], Loss: 20.1460\n",
      "Epoch [14/20], Loss: 19.3784\n",
      "Epoch [15/20], Loss: 18.5797\n",
      "Epoch [16/20], Loss: 18.0732\n",
      "Epoch [17/20], Loss: 17.6600\n",
      "Epoch [18/20], Loss: 17.6032\n",
      "Epoch [19/20], Loss: 17.2177\n",
      "Epoch [20/20], Loss: 17.3905\n",
      "Train Accuracy: 74.10%\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 50\n",
    "batches = model_1_batches(train_corpus, batch_size, max_len)\n",
    "\n",
    "model_1 = BiLSTMClassifier(input_size, hidden_size, num_layers=2, num_classes=3).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        model_1.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_1(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "  correct=0\n",
    "  total=0\n",
    "  for batch in batches:\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "    outputs = model_1(sentence_vectors)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += polarities.size(0)\n",
    "    correct += (predicted == polarities).sum().item()\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV22-u2ucXQa"
   },
   "source": [
    "Accuracy Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSgv1CGWssqJ",
    "outputId": "dcb18c43-d748-48e4-db3c-1152bad5fca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.15%\n"
     ]
    }
   ],
   "source": [
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "  correct=0\n",
    "  total=0\n",
    "  batches = model_1_batches(test_corpus, batch_size, max_len)\n",
    "  for batch in batches:\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "    outputs = model_1(sentence_vectors)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += polarities.size(0)\n",
    "    correct += (predicted == polarities).sum().item()\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmbtMghHiUhS"
   },
   "source": [
    "### Training: with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5E4gWehiZil",
    "outputId": "d8d5132f-2bdc-436c-c6c2-8f16d09b1225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.7575\n",
      "Epoch [2/20], Loss: 28.8167\n",
      "Epoch [3/20], Loss: 27.6265\n",
      "Epoch [4/20], Loss: 25.6615\n",
      "Epoch [5/20], Loss: 24.8092\n",
      "Epoch [6/20], Loss: 25.6300\n",
      "Epoch [7/20], Loss: 24.4182\n",
      "Epoch [8/20], Loss: 24.0011\n",
      "Epoch [9/20], Loss: 23.6244\n",
      "Epoch [10/20], Loss: 23.5575\n",
      "Epoch [11/20], Loss: 23.4395\n",
      "Epoch [12/20], Loss: 23.0348\n",
      "Epoch [13/20], Loss: 22.7402\n",
      "Epoch [14/20], Loss: 22.4790\n",
      "Epoch [15/20], Loss: 22.3577\n",
      "Epoch [16/20], Loss: 22.2226\n",
      "Epoch [17/20], Loss: 21.7619\n",
      "Epoch [18/20], Loss: 21.6676\n",
      "Epoch [19/20], Loss: 21.3935\n",
      "Epoch [20/20], Loss: 21.1553\n",
      "Train Accuracy: 65.35%\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus_sww, batch_size, max_len)\n",
    "\n",
    "model_1_sww = BiLSTMClassifier(input_size, hidden_size, num_layers=2, num_classes=3).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_1_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        model_1_sww.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_1_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "model_1_sww.eval()\n",
    "with torch.no_grad():\n",
    "  correct=0\n",
    "  total=0\n",
    "  for batch in batches:\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "    outputs = model_1_sww(sentence_vectors)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += polarities.size(0)\n",
    "    correct += (predicted == polarities).sum().item()\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heQmOJw1pfFG",
    "outputId": "beb9ac26-15a4-4afb-eadd-7243cc4dea7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 62.49%\n"
     ]
    }
   ],
   "source": [
    "model_1_sww.eval()\n",
    "with torch.no_grad():\n",
    "  correct=0\n",
    "  total=0\n",
    "  batches = model_1_batches(test_corpus_sww, batch_size, max_len)\n",
    "  for batch in batches:\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "    outputs = model_1_sww(sentence_vectors)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += polarities.size(0)\n",
    "    correct += (predicted == polarities).sum().item()\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIo1O5kAtnlB"
   },
   "source": [
    "Polarit probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIUqzAFLse1d"
   },
   "outputs": [],
   "source": [
    "model_1_sww.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    probabilities = []\n",
    "    test_batches = model_1_batches(test_corpus_sww, batch_size, max_len)\n",
    "\n",
    "    for batch in test_batches:\n",
    "        inputs, labels = zip(*batch)\n",
    "        inputs = torch.stack(inputs).to(device)\n",
    "        labels = torch.stack(labels).to(device)\n",
    "\n",
    "        outputs = model_1_sww(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        probabilities.extend(probs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaXxL-3dtH-V",
    "outputId": "133270df-5fa4-4f01-d066-b911638446d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03656283, 0.80860645, 0.15483068], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhvFFlGKCve-"
   },
   "source": [
    "### Training: with custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNs4WWfg_1FX",
    "outputId": "0009aa34-6d3e-4afb-ab9d-62de02d671b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.3798\n",
      "Epoch [2/20], Loss: 26.4379\n",
      "Epoch [3/20], Loss: 24.7232\n",
      "Epoch [4/20], Loss: 23.7791\n",
      "Epoch [5/20], Loss: 23.1937\n",
      "Epoch [6/20], Loss: 22.7127\n",
      "Epoch [7/20], Loss: 22.3200\n",
      "Epoch [8/20], Loss: 21.9679\n",
      "Epoch [9/20], Loss: 21.5947\n",
      "Epoch [10/20], Loss: 21.1733\n",
      "Epoch [11/20], Loss: 20.7551\n",
      "Epoch [12/20], Loss: 20.4969\n",
      "Epoch [13/20], Loss: 20.1070\n",
      "Epoch [14/20], Loss: 19.7192\n",
      "Epoch [15/20], Loss: 19.3583\n",
      "Epoch [16/20], Loss: 19.1352\n",
      "Epoch [17/20], Loss: 18.6326\n",
      "Epoch [18/20], Loss: 17.9727\n",
      "Epoch [19/20], Loss: 17.4438\n",
      "Epoch [20/20], Loss: 17.3132\n",
      "Train Accuracy: 67.83%\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_1_c_sww = BiLSTMClassifier(input_size, hidden_size, num_layers=2, num_classes=3).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_1_c_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        model_1_c_sww.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_1_c_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "model_1_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "  correct=0\n",
    "  total=0\n",
    "  for batch in batches:\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "    outputs = model_1_c_sww(sentence_vectors)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += polarities.size(0)\n",
    "    correct += (predicted == polarities).sum().item()\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTXjDNxl7C4m"
   },
   "source": [
    "## Bi-LSTM Varient-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdlOIyWkkf2W"
   },
   "source": [
    "### Batch Creation for Varient-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "_XwkN_BFd0uP"
   },
   "outputs": [],
   "source": [
    "def model_2_batches(data, batch_size, max_len):\n",
    "    batches = []\n",
    "    polarity_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = []\n",
    "\n",
    "        for j in range(i, min(i + batch_size, len(data))):\n",
    "            sentence = data[j][0]\n",
    "            aspect = data[j][1]\n",
    "            polarity = data[j][2]\n",
    "            embedding_list = []\n",
    "            for word in sentence:\n",
    "                if str(word) in word_vectors:\n",
    "                    embedding_list.append(word_vectors[word])\n",
    "                else:\n",
    "                    embedding_list.append(np.zeros(50))\n",
    "\n",
    "            for _ in range(max_len - len(embedding_list)):\n",
    "                embedding_list.append(np.zeros(50))\n",
    "\n",
    "            sentence_vectors = torch.tensor(embedding_list, dtype=torch.float32)\n",
    "            aspect_vector = torch.tensor(word_vectors[aspect], dtype=torch.float32)\n",
    "            polarity = torch.tensor(polarity_map[polarity], dtype=torch.long)\n",
    "\n",
    "            batch.append((sentence_vectors, aspect_vector, polarity))\n",
    "\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-6zMJHBmRzh"
   },
   "source": [
    "### Bi-LSTM Model\n",
    "\n",
    "Input format for the model: \"Sentence\".\n",
    "\n",
    "Adding \"Aspect\" to the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofKSWnE0mQ6y"
   },
   "outputs": [],
   "source": [
    "# Define the BiLSTM model\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2 + input_size, num_classes)\n",
    "\n",
    "    def forward(self, sentence, aspect):\n",
    "        _, (hidden, _) = self.lstm(sentence)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        output = torch.cat((hidden, aspect), dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRYzC_pFZq6v"
   },
   "source": [
    "### Training: without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYmLR4p7Zpzy",
    "outputId": "edf8b0ef-cec5-42cb-ffc4-914837435d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 28.0405\n",
      "Epoch [2/20], Loss: 25.1489\n",
      "Epoch [3/20], Loss: 23.9537\n",
      "Epoch [4/20], Loss: 23.4113\n",
      "Epoch [5/20], Loss: 23.0241\n",
      "Epoch [6/20], Loss: 22.6788\n",
      "Epoch [7/20], Loss: 22.3587\n",
      "Epoch [8/20], Loss: 22.0469\n",
      "Epoch [9/20], Loss: 21.7164\n",
      "Epoch [10/20], Loss: 21.4914\n",
      "Epoch [11/20], Loss: 21.3229\n",
      "Epoch [12/20], Loss: 21.2114\n",
      "Epoch [13/20], Loss: 20.8062\n",
      "Epoch [14/20], Loss: 20.7288\n",
      "Epoch [15/20], Loss: 20.4577\n",
      "Epoch [16/20], Loss: 20.5390\n",
      "Epoch [17/20], Loss: 20.2530\n",
      "Epoch [18/20], Loss: 20.0805\n",
      "Epoch [19/20], Loss: 20.0419\n",
      "Epoch [20/20], Loss: 19.7239\n",
      "Train Accuracy: 69.08\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Create batches\n",
    "max_len = 50\n",
    "batches = model_2_batches(train_corpus,batch_size, max_len)\n",
    "\n",
    "# Initialize the model\n",
    "model_2 = BiLSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        model_2.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_2(sentence_vectors, aspect_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set after training\n",
    "model_2.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs = model_2(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCR-Z7CeqwQv",
    "outputId": "9efcb488-3e60-4c99-feb5-1b83f32a1a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 63.60\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set after training\n",
    "model_2.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = model_2_batches(test_corpus,batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs = model_2(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SfZke1ap8E1"
   },
   "source": [
    "###Training: with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjlk2e2rqF-3",
    "outputId": "acdbb642-ad8b-49d1-da1a-62d41ec66043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.0724\n",
      "Epoch [2/20], Loss: 26.0974\n",
      "Epoch [3/20], Loss: 24.7151\n",
      "Epoch [4/20], Loss: 23.8732\n",
      "Epoch [5/20], Loss: 23.3199\n",
      "Epoch [6/20], Loss: 22.9947\n",
      "Epoch [7/20], Loss: 22.6603\n",
      "Epoch [8/20], Loss: 22.4492\n",
      "Epoch [9/20], Loss: 22.2086\n",
      "Epoch [10/20], Loss: 21.9903\n",
      "Epoch [11/20], Loss: 21.7906\n",
      "Epoch [12/20], Loss: 21.5921\n",
      "Epoch [13/20], Loss: 21.6121\n",
      "Epoch [14/20], Loss: 21.7550\n",
      "Epoch [15/20], Loss: 21.1712\n",
      "Epoch [16/20], Loss: 20.7924\n",
      "Epoch [17/20], Loss: 20.3167\n",
      "Epoch [18/20], Loss: 20.0311\n",
      "Epoch [19/20], Loss: 19.7812\n",
      "Epoch [20/20], Loss: 19.7956\n",
      "Train Accuracy: 68.27\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "# dropout_rate=0.01\n",
    "\n",
    "# Create batches\n",
    "max_len = 70\n",
    "batches = model_2_batches(train_corpus_sww,batch_size, max_len)\n",
    "\n",
    "# Initialize the model\n",
    "model_2_sww = BiLSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        model_2_sww.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_2_sww(sentence_vectors, aspect_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set after training\n",
    "model_2_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs = model_2_sww(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo0lY0xwqkVA",
    "outputId": "3b7ba5fa-edf6-48a2-c229-14e333796567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.59\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set after training\n",
    "model_2_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = model_2_batches(test_corpus_sww,batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs = model_2_sww(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p7QNB-dGvug"
   },
   "source": [
    "### Training: with custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZXfqbV2G0nG",
    "outputId": "f7eb0c70-1407-4103-b0d9-191230d47672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 28.2716\n",
      "Epoch [2/20], Loss: 25.1602\n",
      "Epoch [3/20], Loss: 23.8001\n",
      "Epoch [4/20], Loss: 23.2014\n",
      "Epoch [5/20], Loss: 22.8009\n",
      "Epoch [6/20], Loss: 22.4383\n",
      "Epoch [7/20], Loss: 22.1447\n",
      "Epoch [8/20], Loss: 21.8680\n",
      "Epoch [9/20], Loss: 21.5618\n",
      "Epoch [10/20], Loss: 21.2600\n",
      "Epoch [11/20], Loss: 21.1880\n",
      "Epoch [12/20], Loss: 21.2796\n",
      "Epoch [13/20], Loss: 20.9506\n",
      "Epoch [14/20], Loss: 20.5855\n",
      "Epoch [15/20], Loss: 20.3031\n",
      "Epoch [16/20], Loss: 20.2503\n",
      "Epoch [17/20], Loss: 20.2448\n",
      "Epoch [18/20], Loss: 19.6396\n",
      "Epoch [19/20], Loss: 19.0954\n",
      "Epoch [20/20], Loss: 18.4546\n",
      "Train Accuracy: 71.73\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create batches\n",
    "max_len = 70\n",
    "batches = model_2_batches(train_corpus_c_sww,batch_size, max_len)\n",
    "\n",
    "# Initialize the model\n",
    "model_2_c_sww = BiLSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2_c_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        model_2_c_sww.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_2_c_sww(sentence_vectors, aspect_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set after training\n",
    "model_2_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs = model_2_c_sww(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EZbPbLhEt8J"
   },
   "source": [
    "## Varient-3 with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoOJ-pvlM1o6"
   },
   "source": [
    "### Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3r4uVgZ6vQi"
   },
   "outputs": [],
   "source": [
    "# Define the Attention model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 6, bias=False)\n",
    "        self.attn1 = nn.Linear(6*70, 70, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_scores = self.attn(lstm_output)\n",
    "        # attention_scores = [batch size, seq_len, 6]\n",
    "        attention_scores = attention_scores.reshape(attention_scores.size(0), -1)\n",
    "        # attention_scores = [batch size, seq_len * 6]\n",
    "        attention_scores1 = self.attn1(attention_scores)\n",
    "        # attention_scores1 = [batch size, 50]\n",
    "        return F.softmax(attention_scores1, dim=1)\n",
    "\n",
    "# Define the BiLSTM with Attention model\n",
    "class BiLSTMClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional=True):\n",
    "        super(BiLSTMClassifierWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attention = Attention(hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_output, (hidden, _) = self.lstm(x, (h0, c0))\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        weighted = lstm_output * attention_weights\n",
    "        weighted_sum = weighted.sum(dim=1)\n",
    "        out = self.fc(weighted_sum)\n",
    "        return out, attention_weights.squeeze(2)\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxROXGLbJcpQ"
   },
   "source": [
    "### Training: without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wy8ArHBRGyXS",
    "outputId": "a4e76ada-efa3-4ff2-8264-9bdb071c57c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.3527\n",
      "Epoch [2/20], Loss: 25.7507\n",
      "Epoch [3/20], Loss: 24.4521\n",
      "Epoch [4/20], Loss: 23.8902\n",
      "Epoch [5/20], Loss: 23.2521\n",
      "Epoch [6/20], Loss: 22.7966\n",
      "Epoch [7/20], Loss: 22.3701\n",
      "Epoch [8/20], Loss: 22.0151\n",
      "Epoch [9/20], Loss: 21.8995\n",
      "Epoch [10/20], Loss: 21.6770\n",
      "Epoch [11/20], Loss: 20.7789\n",
      "Epoch [12/20], Loss: 20.2713\n",
      "Epoch [13/20], Loss: 19.6003\n",
      "Epoch [14/20], Loss: 18.9773\n",
      "Epoch [15/20], Loss: 18.1774\n",
      "Epoch [16/20], Loss: 17.5170\n",
      "Epoch [17/20], Loss: 16.6622\n",
      "Epoch [18/20], Loss: 15.8507\n",
      "Epoch [19/20], Loss: 15.2593\n",
      "Epoch [20/20], Loss: 13.8569\n",
      "Train Accuracy: 80.24%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus, batch_size, max_len)\n",
    "\n",
    "model = BiLSTMClassifierWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CG5YHLEXMMlb",
    "outputId": "9475e5a4-2da5-4ae0-d72b-b22d26524285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.04%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batches = model_1_batches(test_corpus, batch_size, max_len)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsxR8MWCKN4T"
   },
   "source": [
    "### Training: with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yb_xyXt_JkCD",
    "outputId": "c3a7bbc7-0069-4835-d276-4d622c3b7f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.4683\n",
      "Epoch [2/20], Loss: 26.7994\n",
      "Epoch [3/20], Loss: 25.1570\n",
      "Epoch [4/20], Loss: 24.1504\n",
      "Epoch [5/20], Loss: 23.5594\n",
      "Epoch [6/20], Loss: 23.2561\n",
      "Epoch [7/20], Loss: 23.0231\n",
      "Epoch [8/20], Loss: 22.5391\n",
      "Epoch [9/20], Loss: 22.1239\n",
      "Epoch [10/20], Loss: 21.7084\n",
      "Epoch [11/20], Loss: 21.3101\n",
      "Epoch [12/20], Loss: 20.9042\n",
      "Epoch [13/20], Loss: 20.2082\n",
      "Epoch [14/20], Loss: 19.4745\n",
      "Epoch [15/20], Loss: 18.7946\n",
      "Epoch [16/20], Loss: 18.3754\n",
      "Epoch [17/20], Loss: 17.8967\n",
      "Epoch [18/20], Loss: 16.8899\n",
      "Epoch [19/20], Loss: 16.4395\n",
      "Epoch [20/20], Loss: 16.0178\n",
      "Train Accuracy: 69.92%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus_sww, batch_size, max_len)\n",
    "\n",
    "model_sww = BiLSTMClassifierWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWEY89YZMdct"
   },
   "source": [
    "### Training: with custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbYvvo7kLgKq",
    "outputId": "0804a3ee-339d-4e65-98b8-67c482af3fee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-8fe9946e8c6e>:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  sentence_vectors = torch.tensor(embedding_list, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.8806\n",
      "Epoch [2/20], Loss: 26.9533\n",
      "Epoch [3/20], Loss: 24.5145\n",
      "Epoch [4/20], Loss: 23.6427\n",
      "Epoch [5/20], Loss: 23.0285\n",
      "Epoch [6/20], Loss: 22.6469\n",
      "Epoch [7/20], Loss: 22.2336\n",
      "Epoch [8/20], Loss: 21.8111\n",
      "Epoch [9/20], Loss: 21.4013\n",
      "Epoch [10/20], Loss: 20.8987\n",
      "Epoch [11/20], Loss: 20.5075\n",
      "Epoch [12/20], Loss: 19.8988\n",
      "Epoch [13/20], Loss: 18.9793\n",
      "Epoch [14/20], Loss: 18.3763\n",
      "Epoch [15/20], Loss: 18.0186\n",
      "Epoch [16/20], Loss: 18.4016\n",
      "Epoch [17/20], Loss: 17.1485\n",
      "Epoch [18/20], Loss: 15.6702\n",
      "Epoch [19/20], Loss: 14.4943\n",
      "Epoch [20/20], Loss: 14.3356\n",
      "Train Accuracy: 80.51%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_c_sww = BiLSTMClassifierWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_c_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model_c_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_c_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzO_wQFezVz8"
   },
   "source": [
    "#### Visualising Attention on different sentesnces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdcKPFVAi5v3",
    "outputId": "0d76ef35-184b-4d7a-fd5d-3ee8d16a8cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 0.009234804660081863\n",
      "atmosphere: 0.0024860305711627007\n",
      "wa: 0.00414225272834301\n",
      "wonderful: 0.012421343475580215\n",
      "however: 0.03221551701426506\n",
      "the: 0.01490062940865755\n",
      "service: 0.02014872059226036\n",
      "and: 0.0038326054345816374\n",
      "food: 0.009260198101401329\n",
      "were: 0.0016536545008420944\n",
      "not: 0.002169385552406311\n",
      "food: 0.0071614691987633705\n"
     ]
    }
   ],
   "source": [
    "data = test_corpus_sww\n",
    "sentence = data[51][0]\n",
    "aspect = data[51][1]\n",
    "polarity = data[51][2]\n",
    "\n",
    "embedding_list = []\n",
    "word_list = []  # Store the words in the sentence\n",
    "\n",
    "for word in sentence:\n",
    "    if str(word) in word_vectors:\n",
    "        embedding_list.append(word_vectors[word])\n",
    "        word_list.append(word)  # Append the word to the word_list\n",
    "    else:\n",
    "        embedding_list.append(np.zeros(50))\n",
    "        word_list.append(word)  # Append the word to the word_list\n",
    "\n",
    "embedding_list.append(word_vectors[aspect])\n",
    "word_list.append(aspect)  # Append the aspect to the word_list\n",
    "\n",
    "for _ in range(max_len - len(embedding_list)):\n",
    "    embedding_list.append(np.zeros(50))\n",
    "    word_list.append('')  # Append an empty string for padding\n",
    "\n",
    "batch = []\n",
    "polarity_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "sentence_vectors = torch.tensor(embedding_list, dtype=torch.float32)\n",
    "polarity = torch.tensor(polarity_map[polarity], dtype=torch.long)\n",
    "batch.append((sentence_vectors, polarity))\n",
    "\n",
    "model_sww.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "    outputs, attention_weights = model_sww(sentence_vectors)\n",
    "    word_prob={}\n",
    "\n",
    "\n",
    "    for word, attention in zip(word_list, attention_weights.squeeze()):\n",
    "      if word.strip():\n",
    "            print(f\"{word}: {attention.item()}\")\n",
    "            word_prob[word]=attention.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJ96SSvakd7J",
    "outputId": "0056ee5d-c295-4fda-93eb-90e9a0ce3c05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The atmosphere was wonderful, however the service and food were not.',\n",
       "       'service', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = test_data.to_numpy()\n",
    "first[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "yhq1t1q8nIz4",
    "outputId": "e8fed413-ef1b-4aa0-e9f2-3378757bcbd1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x100 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "word_prob_df = pd.DataFrame(list(word_prob.items()), columns=['Word', 'Attention'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,1))\n",
    "sns.heatmap([word_prob_df['Attention']], annot=word_prob_df[['Word']].T.values, fmt='', cmap='Blues', cbar=True, cbar_kws={'orientation':'horizontal'}, ax=ax)\n",
    "\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_ticks([0.02, 0.04, 0.06])\n",
    "cbar.set_ticklabels(['Low', 'Medium', 'High'])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPYdTaBSzL6K",
    "outputId": "4f0ff3c9-480a-4772-c8e5-257e63560d4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The atmosphere was wonderful, however the service and food were not.',\n",
       "       'food', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "2NzaXcAHymqZ",
    "outputId": "c0932106-8edb-409a-9b70-99f088203eaa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x100 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "word_prob_df = pd.DataFrame(list(word_prob.items()), columns=['Word', 'Attention'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,1))\n",
    "sns.heatmap([word_prob_df['Attention']], annot=word_prob_df[['Word']].T.values, fmt='', cmap='Blues', cbar=True, cbar_kws={'orientation':'horizontal'}, ax=ax)\n",
    "\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_ticks([0.01, 0.02, 0.03])\n",
    "cbar.set_ticklabels(['Low', 'Medium', 'High'])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v-wHFgHUenB"
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Used training set to training and validation set for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MqM7N8c1J6k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSCvgcTAee-D"
   },
   "outputs": [],
   "source": [
    "# Define the Attention model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 6, bias=False)\n",
    "        self.attn1 = nn.Linear(6*70, 70, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_scores = self.attn(lstm_output)\n",
    "        # attention_scores = [batch size, seq_len, 6]\n",
    "        attention_scores = attention_scores.reshape(attention_scores.size(0), -1)\n",
    "        # attention_scores = [batch size, seq_len * 6]\n",
    "        #print(attention_scores.shape)\n",
    "        attention_scores1 = self.attn1(attention_scores)\n",
    "        # attention_scores1 = [batch size, 50]\n",
    "        return F.softmax(attention_scores1, dim=1)\n",
    "\n",
    "# Define the BiLSTM with Attention model\n",
    "class BiLSTMClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_classes, bidirectional=True):\n",
    "        super(BiLSTMClassifierWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=self.dropout, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attention = Attention(hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_output, (hidden, _) = self.lstm(x, (h0, c0))\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        weighted = lstm_output * attention_weights\n",
    "        weighted_sum = weighted.sum(dim=1)\n",
    "        out = self.fc(weighted_sum)\n",
    "        return out, attention_weights.squeeze(2)\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-eOBITrehf3",
    "outputId": "81cd74e5-901d-4bf7-ea96-0b4a986df9a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 1, Learning rate: 0.1, hidden size: 64, Dropout: 0.1, Accuracy: 43.6937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 1, Learning rate: 0.1, hidden size: 64, Dropout: 0.2, Accuracy: 63.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 1, Learning rate: 0.1, hidden size: 64, Dropout: 0.3, Accuracy: 43.6937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 1, Learning rate: 0.1, hidden size: 64, Dropout: 0.4, Accuracy: 43.6937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 1, Learning rate: 0.1, hidden size: 64, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 128, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 128, Dropout: 0.2, Accuracy: 58.8964\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 128, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 128, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 128, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 256, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 256, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 256, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 256, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 256, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 512, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 512, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 512, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 512, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.1, hidden size: 512, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 64, Dropout: 0.1, Accuracy: 60.4730\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 64, Dropout: 0.2, Accuracy: 60.9234\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 64, Dropout: 0.3, Accuracy: 60.2477\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 64, Dropout: 0.4, Accuracy: 63.6261\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 64, Dropout: 0.5, Accuracy: 64.9775\n",
      "\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 128, Dropout: 0.1, Accuracy: 61.5991\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 128, Dropout: 0.2, Accuracy: 62.8378\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 128, Dropout: 0.3, Accuracy: 60.1351\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 128, Dropout: 0.4, Accuracy: 63.6261\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 128, Dropout: 0.5, Accuracy: 62.0495\n",
      "\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 256, Dropout: 0.1, Accuracy: 45.0450\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 256, Dropout: 0.2, Accuracy: 59.4595\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 256, Dropout: 0.3, Accuracy: 59.7973\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 256, Dropout: 0.4, Accuracy: 61.1486\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 256, Dropout: 0.5, Accuracy: 62.1622\n",
      "\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 512, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 512, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 512, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 512, Dropout: 0.4, Accuracy: 46.1712\n",
      "Layers: 1, Learning rate: 0.01, hidden size: 512, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 64, Dropout: 0.1, Accuracy: 63.5135\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 64, Dropout: 0.2, Accuracy: 61.7117\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 64, Dropout: 0.3, Accuracy: 60.6982\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 64, Dropout: 0.4, Accuracy: 61.1486\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 64, Dropout: 0.5, Accuracy: 62.0495\n",
      "\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 128, Dropout: 0.1, Accuracy: 61.7117\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 128, Dropout: 0.2, Accuracy: 60.6982\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 128, Dropout: 0.3, Accuracy: 60.0225\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 128, Dropout: 0.4, Accuracy: 60.2477\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 128, Dropout: 0.5, Accuracy: 61.1486\n",
      "\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 256, Dropout: 0.1, Accuracy: 64.5270\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 256, Dropout: 0.2, Accuracy: 61.8243\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 256, Dropout: 0.3, Accuracy: 61.1486\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 256, Dropout: 0.4, Accuracy: 62.2748\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 256, Dropout: 0.5, Accuracy: 59.5721\n",
      "\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 512, Dropout: 0.1, Accuracy: 59.1216\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 512, Dropout: 0.2, Accuracy: 57.3198\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 512, Dropout: 0.3, Accuracy: 62.0495\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 512, Dropout: 0.4, Accuracy: 59.2342\n",
      "Layers: 1, Learning rate: 0.001, hidden size: 512, Dropout: 0.5, Accuracy: 57.3198\n",
      "\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 64, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 64, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 64, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 64, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 64, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 128, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 128, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 128, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 128, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 128, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 256, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 256, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 256, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 256, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 256, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 512, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 512, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 512, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 512, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.1, hidden size: 512, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 64, Dropout: 0.1, Accuracy: 61.4865\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 64, Dropout: 0.2, Accuracy: 64.9775\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 64, Dropout: 0.3, Accuracy: 63.5135\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 64, Dropout: 0.4, Accuracy: 64.7523\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 64, Dropout: 0.5, Accuracy: 63.2883\n",
      "\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 128, Dropout: 0.1, Accuracy: 61.0360\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 128, Dropout: 0.2, Accuracy: 63.1757\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 128, Dropout: 0.3, Accuracy: 63.0631\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 128, Dropout: 0.4, Accuracy: 63.8514\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 128, Dropout: 0.5, Accuracy: 64.4144\n",
      "\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 256, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 256, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 256, Dropout: 0.3, Accuracy: 61.5991\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 256, Dropout: 0.4, Accuracy: 59.0090\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 256, Dropout: 0.5, Accuracy: 60.3604\n",
      "\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 512, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 512, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 512, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 512, Dropout: 0.4, Accuracy: 44.2568\n",
      "Layers: 2, Learning rate: 0.01, hidden size: 512, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 64, Dropout: 0.1, Accuracy: 63.4009\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 64, Dropout: 0.2, Accuracy: 62.6126\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 64, Dropout: 0.3, Accuracy: 63.5135\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 64, Dropout: 0.4, Accuracy: 62.3874\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 64, Dropout: 0.5, Accuracy: 63.6261\n",
      "\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 128, Dropout: 0.1, Accuracy: 63.2883\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 128, Dropout: 0.2, Accuracy: 60.0225\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 128, Dropout: 0.3, Accuracy: 62.7252\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 128, Dropout: 0.4, Accuracy: 62.2748\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 128, Dropout: 0.5, Accuracy: 58.8964\n",
      "\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 256, Dropout: 0.1, Accuracy: 61.0360\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 256, Dropout: 0.2, Accuracy: 59.1216\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 256, Dropout: 0.3, Accuracy: 60.3604\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 256, Dropout: 0.4, Accuracy: 62.5000\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 256, Dropout: 0.5, Accuracy: 60.2477\n",
      "\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 512, Dropout: 0.1, Accuracy: 62.2748\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 512, Dropout: 0.2, Accuracy: 56.9820\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 512, Dropout: 0.3, Accuracy: 58.5586\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 512, Dropout: 0.4, Accuracy: 60.5856\n",
      "Layers: 2, Learning rate: 0.001, hidden size: 512, Dropout: 0.5, Accuracy: 58.3333\n",
      "\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 64, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 64, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 64, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 64, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 64, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 128, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 128, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 128, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 128, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 128, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 256, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 256, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 256, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 256, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 256, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 512, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 512, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 512, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 512, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.1, hidden size: 512, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 64, Dropout: 0.1, Accuracy: 64.4144\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 64, Dropout: 0.2, Accuracy: 64.9775\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 64, Dropout: 0.3, Accuracy: 61.4865\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 64, Dropout: 0.4, Accuracy: 64.8649\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 64, Dropout: 0.5, Accuracy: 63.6261\n",
      "\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 128, Dropout: 0.1, Accuracy: 61.4865\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 128, Dropout: 0.2, Accuracy: 64.8649\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 128, Dropout: 0.3, Accuracy: 63.9640\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 128, Dropout: 0.4, Accuracy: 43.9189\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 128, Dropout: 0.5, Accuracy: 64.7523\n",
      "\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 256, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 256, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 256, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 256, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 256, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 512, Dropout: 0.1, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 512, Dropout: 0.2, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 512, Dropout: 0.3, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 512, Dropout: 0.4, Accuracy: 43.6937\n",
      "Layers: 3, Learning rate: 0.01, hidden size: 512, Dropout: 0.5, Accuracy: 43.6937\n",
      "\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 64, Dropout: 0.1, Accuracy: 61.8243\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 64, Dropout: 0.2, Accuracy: 60.5856\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 64, Dropout: 0.3, Accuracy: 64.3018\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 64, Dropout: 0.4, Accuracy: 62.7252\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 64, Dropout: 0.5, Accuracy: 61.8243\n",
      "\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 128, Dropout: 0.1, Accuracy: 62.2748\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 128, Dropout: 0.2, Accuracy: 61.2613\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 128, Dropout: 0.3, Accuracy: 61.5991\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 128, Dropout: 0.4, Accuracy: 59.3468\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 128, Dropout: 0.5, Accuracy: 63.0631\n",
      "\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 256, Dropout: 0.1, Accuracy: 60.5856\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 256, Dropout: 0.2, Accuracy: 58.7838\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 256, Dropout: 0.3, Accuracy: 62.0495\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 256, Dropout: 0.4, Accuracy: 60.2477\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 256, Dropout: 0.5, Accuracy: 64.0766\n",
      "\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 512, Dropout: 0.1, Accuracy: 62.3874\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 512, Dropout: 0.2, Accuracy: 58.8964\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 512, Dropout: 0.3, Accuracy: 54.5045\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 512, Dropout: 0.4, Accuracy: 61.0360\n",
      "Layers: 3, Learning rate: 0.001, hidden size: 512, Dropout: 0.5, Accuracy: 59.3468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_sizes = (64, 128, 256, 512)\n",
    "num_layers_list = (1, 2, 3)\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rates = (0.1, 0.01, 0.001)\n",
    "dropouts = (0.1, 0.2, 0.3, 0.4, 0.5)\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "for num_layers in num_layers_list:  # Iterate over num_layers_list\n",
    "  for learning_rate in learning_rates:\n",
    "    for hidden_size in hidden_sizes:\n",
    "      for dropout in dropouts:\n",
    "      # Initialize the model\n",
    "        model_c_sww = BiLSTMClassifierWithAttention(input_size, hidden_size, num_layers, dropout, num_classes).to(device)\n",
    "\n",
    "      # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model_c_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "          loss_model = 0\n",
    "          for batch in batches:\n",
    "            sentence_vectors, polarities = zip(*batch)\n",
    "            sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "            polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "            outputs, attention_weights = model_c_sww(sentence_vectors)\n",
    "            loss = criterion(outputs, polarities)\n",
    "            loss_model += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        v_batches = model_1_batches(val_corpus_c_sww, batch_size, max_len)\n",
    "        model_c_sww.eval()\n",
    "        with torch.no_grad():\n",
    "          correct = 0\n",
    "          total = 0\n",
    "          for batch in v_batches:\n",
    "            sentence_vectors, polarities = zip(*batch)\n",
    "            sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "            polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "            outputs, attention_weights = model_c_sww(sentence_vectors)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += polarities.size(0)\n",
    "            correct += (predicted == polarities).sum().item()\n",
    "\n",
    "          accuracy = 100 * correct / total\n",
    "          print(f'Layers: {num_layers}, Learning rate: {learning_rate}, hidden size: {hidden_size}, Dropout: {dropout}, Accuracy: {accuracy:.4f}')\n",
    "      print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVLzR0LDRP0o"
   },
   "source": [
    "## Final models\n",
    "\n",
    "Using hyperparameters selected after tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCexZcI2RUdU"
   },
   "source": [
    "### Bi-LSTM Varient-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPNVl25GRSqN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the BiLSTM model\n",
    "class BiLSTMClassifierModel1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_classes):\n",
    "        super(BiLSTMClassifierModel1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=self.dropout, batch_first =True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_size,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,(h_n,c_n) = self.lstm(x)\n",
    "        hidden_out = torch.cat((h_n[2,:,:],h_n[3,:,:]),1)\n",
    "        output = self.fc(hidden_out)\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dyynvd4It1lg",
    "outputId": "edb2c131-0da3-49f8-b198-5be12b473541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 28.9137\n",
      "Epoch [2/20], Loss: 25.3022\n",
      "Epoch [3/20], Loss: 23.6056\n",
      "Epoch [4/20], Loss: 22.6999\n",
      "Epoch [5/20], Loss: 21.7496\n",
      "Epoch [6/20], Loss: 20.8045\n",
      "Epoch [7/20], Loss: 21.1444\n",
      "Epoch [8/20], Loss: 21.8652\n",
      "Epoch [9/20], Loss: 19.9216\n",
      "Epoch [10/20], Loss: 18.7877\n",
      "Epoch [11/20], Loss: 17.0535\n",
      "Epoch [12/20], Loss: 15.9630\n",
      "Epoch [13/20], Loss: 15.1543\n",
      "Epoch [14/20], Loss: 14.5930\n",
      "Epoch [15/20], Loss: 14.4215\n",
      "Epoch [16/20], Loss: 13.4263\n",
      "Epoch [17/20], Loss: 12.4576\n",
      "Epoch [18/20], Loss: 12.2135\n",
      "Epoch [19/20], Loss: 13.5794\n",
      "Epoch [20/20], Loss: 12.2847\n",
      "Train Accuracy: 86.83%\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "dropout=0.5\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_1_c_sww = BiLSTMClassifierModel1(input_size, hidden_size, num_layers, dropout, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_1_c_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        model_1_c_sww.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_1_c_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "model_1_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "  correct=0\n",
    "  total=0\n",
    "  for batch in batches:\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "    outputs = model_1_c_sww(sentence_vectors)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += polarities.size(0)\n",
    "    correct += (predicted == polarities).sum().item()\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYs12SkArlge",
    "outputId": "142bbe2b-4afe-4a1a-a939-d214d338a404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4856434e-01 7.8681228e-04 8.5064882e-01]]\n"
     ]
    }
   ],
   "source": [
    "data = test_corpus_c_sww\n",
    "sentence = data[51][0]\n",
    "aspect = data[51][1]\n",
    "polarity = data[51][2]\n",
    "\n",
    "embedding_list = []\n",
    "word_list = []  # Store the words in the sentence\n",
    "\n",
    "for word in sentence:\n",
    "    if str(word) in word_vectors:\n",
    "        embedding_list.append(word_vectors[word])\n",
    "        word_list.append(word)  # Append the word to the word_list\n",
    "    else:\n",
    "        embedding_list.append(np.zeros(50))\n",
    "        word_list.append(word)  # Append the word to the word_list\n",
    "\n",
    "embedding_list.append(word_vectors[aspect])\n",
    "word_list.append(aspect)  # Append the aspect to the word_list\n",
    "\n",
    "# Pad the embedding_list to max_len\n",
    "padding_length = max_len - len(embedding_list)\n",
    "embedding_list.extend([np.zeros(50)] * padding_length)\n",
    "word_list.extend([''] * padding_length)  # Pad word_list with empty strings\n",
    "\n",
    "batch = []\n",
    "polarity_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "sentence_vectors = torch.tensor(embedding_list, dtype=torch.float32)\n",
    "polarity = torch.tensor(polarity_map[polarity], dtype=torch.long)\n",
    "batch.append((sentence_vectors, polarity))\n",
    "\n",
    "model_1_c_sww.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # probabilities = []\n",
    "\n",
    "    inputs, labels = zip(*batch)\n",
    "    inputs = torch.stack(inputs).to(device)\n",
    "    labels = torch.stack(labels).to(device)\n",
    "\n",
    "    outputs = model_1_c_sww(inputs)\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    probabilities = probs.cpu().numpy()\n",
    "\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXEeFOk2UF3V"
   },
   "source": [
    "### Bi-LSTM Varient-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "-qnHSGphUK0N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the BiLSTM model\n",
    "class BiLSTMClassifierModel2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_classes):\n",
    "        super(BiLSTMClassifierModel2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=self.dropout, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2 + input_size, num_classes)\n",
    "\n",
    "    def forward(self, sentence, aspect):\n",
    "        _, (hidden, _) = self.lstm(sentence)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        output = torch.cat((hidden, aspect), dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwnYJsIUUXl9",
    "outputId": "32d4bae6-49dc-4b4e-f2b1-dca30c5737ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 26.0158\n",
      "Epoch [2/20], Loss: 24.0401\n",
      "Epoch [3/20], Loss: 22.9765\n",
      "Epoch [4/20], Loss: 22.4440\n",
      "Epoch [5/20], Loss: 21.8760\n",
      "Epoch [6/20], Loss: 20.9447\n",
      "Epoch [7/20], Loss: 20.4610\n",
      "Epoch [8/20], Loss: 20.3974\n",
      "Epoch [9/20], Loss: 19.6621\n",
      "Epoch [10/20], Loss: 19.0964\n",
      "Epoch [11/20], Loss: 18.2859\n",
      "Epoch [12/20], Loss: 17.8752\n",
      "Epoch [13/20], Loss: 18.0879\n",
      "Epoch [14/20], Loss: 18.1414\n",
      "Epoch [15/20], Loss: 18.2550\n",
      "Epoch [16/20], Loss: 17.3158\n",
      "Epoch [17/20], Loss: 16.8042\n",
      "Epoch [18/20], Loss: 16.6171\n",
      "Epoch [19/20], Loss: 16.2494\n",
      "Epoch [20/20], Loss: 16.0966\n",
      "Train Accuracy: 74.96%\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "dropout=0.5\n",
    "\n",
    "# Create batches\n",
    "max_len = 70\n",
    "batches = model_2_batches(train_corpus_c_sww,batch_size, max_len)\n",
    "\n",
    "# Initialize the model\n",
    "model_2_c_sww = BiLSTMClassifierModel2(input_size, hidden_size, num_layers, dropout, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2_c_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        model_2_c_sww.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_2_c_sww(sentence_vectors, aspect_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set after training\n",
    "model_2_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs = model_2_c_sww(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxbYFyGGWMkN"
   },
   "source": [
    "### Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "LsIDqygYU7ld"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Attention model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 6, bias=False)\n",
    "        self.attn1 = nn.Linear(6*70, 70, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output = [batch size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attn(lstm_output)\n",
    "        # attention_scores = [batch size, seq_len, 6]\n",
    "        attention_scores = attention_scores.reshape(attention_scores.size(0), -1)\n",
    "        # attention_scores = [batch size, seq_len * 6]\n",
    "        attention_scores1 = self.attn1(attention_scores)\n",
    "        # attention_scores1 = [batch size, 50]\n",
    "        return F.softmax(attention_scores1, dim=1)\n",
    "\n",
    "# Define the BiLSTM with Attention model\n",
    "class BiLSTMClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_classes, bidirectional=True):\n",
    "        super(BiLSTMClassifierWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=self.dropout, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attention = Attention(hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_output, (hidden, _) = self.lstm(x, (h0, c0))\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        weighted = lstm_output * attention_weights\n",
    "        weighted_sum = weighted.sum(dim=1)\n",
    "        out = self.fc(weighted_sum)\n",
    "        return out, attention_weights.squeeze(2)\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7eIAHp_WStm",
    "outputId": "931ec72a-cb03-47c3-d84b-72fbf2e14ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.7559\n",
      "Epoch [2/20], Loss: 25.3015\n",
      "Epoch [3/20], Loss: 24.2526\n",
      "Epoch [4/20], Loss: 22.7004\n",
      "Epoch [5/20], Loss: 21.5958\n",
      "Epoch [6/20], Loss: 20.3563\n",
      "Epoch [7/20], Loss: 18.6652\n",
      "Epoch [8/20], Loss: 17.6790\n",
      "Epoch [9/20], Loss: 17.3212\n",
      "Epoch [10/20], Loss: 16.3397\n",
      "Epoch [11/20], Loss: 14.2315\n",
      "Epoch [12/20], Loss: 11.3497\n",
      "Epoch [13/20], Loss: 11.0165\n",
      "Epoch [14/20], Loss: 10.7412\n",
      "Epoch [15/20], Loss: 9.5820\n",
      "Epoch [16/20], Loss: 7.3028\n",
      "Epoch [17/20], Loss: 8.1856\n",
      "Epoch [18/20], Loss: 7.4596\n",
      "Epoch [19/20], Loss: 5.7055\n",
      "Epoch [20/20], Loss: 5.6769\n",
      "Train Accuracy: 95.42%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "dropout=0.05\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70 # This is not actual max length, change input_size if you change max_len.\n",
    "batches = model_1_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_c_sww = BiLSTMClassifierWithAttention(input_size, hidden_size, num_layers, dropout, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_c_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model_c_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_c_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OTGbrOq5Dw_"
   },
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H17baCev6nSi"
   },
   "source": [
    "### Bi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gfEGdgT5Fu4",
    "outputId": "04c83421-b9f3-49a9-b6e7-af66f74b268d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 29.2215\n",
      "Epoch [2/20], Loss: 26.3207\n",
      "Epoch [3/20], Loss: 24.8450\n",
      "Epoch [4/20], Loss: 24.3356\n",
      "Epoch [5/20], Loss: 24.1349\n",
      "Epoch [6/20], Loss: 23.5552\n",
      "Epoch [7/20], Loss: 22.8603\n",
      "Epoch [8/20], Loss: 22.5607\n",
      "Epoch [9/20], Loss: 21.8998\n",
      "Epoch [10/20], Loss: 21.7747\n",
      "Epoch [11/20], Loss: 21.3539\n",
      "Epoch [12/20], Loss: 20.7929\n",
      "Epoch [13/20], Loss: 20.2869\n",
      "Epoch [14/20], Loss: 19.8432\n",
      "Epoch [15/20], Loss: 19.3274\n",
      "Epoch [16/20], Loss: 19.0057\n",
      "Epoch [17/20], Loss: 18.7914\n",
      "Epoch [18/20], Loss: 18.3500\n",
      "Epoch [19/20], Loss: 17.7715\n",
      "Epoch [20/20], Loss: 17.2484\n",
      "Train Accuracy: 72.91%\n"
     ]
    }
   ],
   "source": [
    "# Define the Attention model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 6, bias=False)\n",
    "        self.attn1 = nn.Linear(6*70, 70, bias=False)\n",
    "\n",
    "    def forward(self, rnn_output):\n",
    "        # lstm_output = [batch size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attn(rnn_output)\n",
    "        # attention_scores = [batch size, seq_len, 6]\n",
    "        attention_scores = attention_scores.reshape(attention_scores.size(0), -1)\n",
    "        # attention_scores = [batch size, seq_len * 6]\n",
    "        #print(attention_scores.shape)\n",
    "        attention_scores1 = self.attn1(attention_scores)\n",
    "        # attention_scores1 = [batch size, 50]\n",
    "        return F.softmax(attention_scores1, dim=1)\n",
    "\n",
    "# Define the BiLSTM with Attention model\n",
    "class BiRNNClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional=True):\n",
    "        super(BiRNNClassifierWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attention = Attention(hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        rnn_output, h_n = self.rnn(x, h0)\n",
    "        attention_weights = self.attention(rnn_output)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        weighted = rnn_output * attention_weights\n",
    "        weighted_sum = weighted.sum(dim=1)\n",
    "        out = self.fc(weighted_sum)\n",
    "        return out, attention_weights.squeeze(2)\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70 # This is not actual max length, change input_size if you change max_len.\n",
    "batches = model_1_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_rnn_sww = BiRNNClassifierWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_rnn_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model_rnn_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model_rnn_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_rnn_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PyR4LZGX2ar7",
    "outputId": "005bb0fa-b442-45d6-f2f5-200f7b5cc693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.49%\n"
     ]
    }
   ],
   "source": [
    "model_rnn_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = model_1_batches(test_corpus_c_sww, batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_rnn_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5EBIToOAB1k"
   },
   "source": [
    "### Bi-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aR3oSFLsAEo8",
    "outputId": "d6ac992d-da15-483a-d0a1-e1ac96def6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 28.7539\n",
      "Epoch [2/20], Loss: 25.0442\n",
      "Epoch [3/20], Loss: 23.6502\n",
      "Epoch [4/20], Loss: 22.9575\n",
      "Epoch [5/20], Loss: 22.5102\n",
      "Epoch [6/20], Loss: 21.9780\n",
      "Epoch [7/20], Loss: 21.4944\n",
      "Epoch [8/20], Loss: 21.0277\n",
      "Epoch [9/20], Loss: 20.4423\n",
      "Epoch [10/20], Loss: 19.6973\n",
      "Epoch [11/20], Loss: 18.8277\n",
      "Epoch [12/20], Loss: 17.9179\n",
      "Epoch [13/20], Loss: 17.0076\n",
      "Epoch [14/20], Loss: 16.0141\n",
      "Epoch [15/20], Loss: 16.0173\n",
      "Epoch [16/20], Loss: 16.3605\n",
      "Epoch [17/20], Loss: 14.6974\n",
      "Epoch [18/20], Loss: 13.3278\n",
      "Epoch [19/20], Loss: 11.9828\n",
      "Epoch [20/20], Loss: 10.9028\n",
      "Train Accuracy: 86.90%\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 6, bias=False)\n",
    "        self.attn1 = nn.Linear(6*70, 70, bias=False)\n",
    "\n",
    "    def forward(self, output):\n",
    "        # lstm_output = [batch size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attn(output)\n",
    "        # attention_scores = [batch size, seq_len, 6]\n",
    "        attention_scores = attention_scores.reshape(attention_scores.size(0), -1)\n",
    "        # attention_scores = [batch size, seq_len * 6]\n",
    "        #print(attention_scores.shape)\n",
    "        attention_scores1 = self.attn1(attention_scores)\n",
    "        # attention_scores1 = [batch size, 50]\n",
    "        return F.softmax(attention_scores1, dim=1)\n",
    "\n",
    "# Define the BiLSTM with Attention model\n",
    "class BiGRUClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional=True):\n",
    "        super(BiGRUClassifierWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attention = Attention(hidden_size * 2 if bidirectional else hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        gru_output, h_n = self.gru(x, h0)\n",
    "        attention_weights = self.attention(gru_output)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        weighted = gru_output * attention_weights\n",
    "        weighted_sum = weighted.sum(dim=1)\n",
    "        out = self.fc(weighted_sum)\n",
    "        return out, attention_weights.squeeze(2)\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_1_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_gru_sww = BiGRUClassifierWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_gru_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model_gru_sww(sentence_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model_gru_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_gru_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqZxuBXo2_Wn",
    "outputId": "73192ccc-2a20-412f-c305-a0220c5478a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 62.93%\n"
     ]
    }
   ],
   "source": [
    "model_gru_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = model_1_batches(test_corpus_c_sww, batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_gru_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cj0K0VGDZ2o"
   },
   "source": [
    "### Bi-RNN with attention by adding aspect with each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "xtydYk5nVqhl"
   },
   "outputs": [],
   "source": [
    "def model_3_batches(data, batch_size, max_len):\n",
    "    batches = []\n",
    "    polarity_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = []\n",
    "\n",
    "        for j in range(i, min(i + batch_size, len(data))):\n",
    "            sentence = data[j][0]\n",
    "            aspect = data[j][1]\n",
    "            polarity = data[j][2]\n",
    "            embedding_list = []\n",
    "            for word in sentence: # Check if word exist in pre-trained word embedding else appending zero vector\n",
    "                if str(word) in word_vectors:\n",
    "                    embedding_list.append(word_vectors[word])\n",
    "                else:\n",
    "                    embedding_list.append(np.zeros(50))\n",
    "\n",
    "            embedding_list.append(word_vectors[aspect])\n",
    "\n",
    "            for _ in range(max_len - len(embedding_list)):\n",
    "                embedding_list.append(np.zeros(50))\n",
    "\n",
    "            sentence_vectors = torch.tensor(embedding_list, dtype=torch.float32)\n",
    "            aspect_vectors = torch.tensor(word_vectors[aspect], dtype=torch.float32)\n",
    "            polarity = torch.tensor(polarity_map[polarity], dtype=torch.long)\n",
    "\n",
    "            batch.append((sentence_vectors, aspect_vectors, polarity))\n",
    "\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2ECL4sqKpse",
    "outputId": "3a27a8cd-3865-4e69-a4d0-a89ee85f8494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 27.6524\n",
      "Epoch [2/20], Loss: 24.8920\n",
      "Epoch [3/20], Loss: 24.1960\n",
      "Epoch [4/20], Loss: 23.8749\n",
      "Epoch [5/20], Loss: 23.3572\n",
      "Epoch [6/20], Loss: 23.0718\n",
      "Epoch [7/20], Loss: 22.7153\n",
      "Epoch [8/20], Loss: 22.4554\n",
      "Epoch [9/20], Loss: 22.0809\n",
      "Epoch [10/20], Loss: 21.5781\n",
      "Epoch [11/20], Loss: 21.2340\n",
      "Epoch [12/20], Loss: 20.7719\n",
      "Epoch [13/20], Loss: 20.3385\n",
      "Epoch [14/20], Loss: 20.0289\n",
      "Epoch [15/20], Loss: 19.7159\n",
      "Epoch [16/20], Loss: 18.9631\n",
      "Epoch [17/20], Loss: 18.6324\n",
      "Epoch [18/20], Loss: 18.3697\n",
      "Epoch [19/20], Loss: 17.7808\n",
      "Epoch [20/20], Loss: 17.4213\n",
      "Train Accuracy: 73.95%\n"
     ]
    }
   ],
   "source": [
    "# Define the Attention model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim + input_size, 6, bias=False)\n",
    "        self.attn1 = nn.Linear(6 * max_len, max_len, bias=False)  # Use max_len instead of hardcoded value\n",
    "\n",
    "    def forward(self, rnn_output, aspect_vectors):\n",
    "    # lstm_output = [batch size, seq_len, hidden_dim]\n",
    "      aspect_vectors = aspect_vectors.unsqueeze(1).expand(-1, rnn_output.size(1), -1)  # Add extra dimension and expand\n",
    "\n",
    "      attention_scores = self.attn(torch.cat((rnn_output, aspect_vectors), dim=2))\n",
    "    # attention_scores = [batch size, seq_len, 6]\n",
    "      attention_scores = attention_scores.reshape(attention_scores.size(0), -1)\n",
    "    # attention_scores = [batch size, seq_len * 6]\n",
    "      attention_scores1 = self.attn1(attention_scores)\n",
    "    # attention_scores1 = [batch size, max_len]\n",
    "      return F.softmax(attention_scores1, dim=1)\n",
    "\n",
    "\n",
    "class BiRNNClassifierWithAttentionAspect(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional=True):\n",
    "        super(BiRNNClassifierWithAttentionAspect, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attention = Attention(hidden_size * 2 if bidirectional else hidden_size, input_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, aspect_vectors):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        rnn_output, h_n = self.rnn(x, h0)\n",
    "        attention_weights = self.attention(rnn_output, aspect_vectors)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        weighted = rnn_output * attention_weights\n",
    "        weighted_sum = weighted.sum(dim=1)\n",
    "        out = self.fc(weighted_sum)\n",
    "        return out, attention_weights.squeeze(2)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_3_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_rnn_sww = BiRNNClassifierWithAttentionAspect(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_rnn_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model_rnn_sww(sentence_vectors, aspect_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model_rnn_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)  # Add aspect_vectors\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)  # Add aspect_vectors\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_rnn_sww(sentence_vectors, aspect_vectors)  # Pass aspect_vectors\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fy9gX_0QgbbM",
    "outputId": "bcbd2415-a3bf-48e3-a5a1-937deb8cc67b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 57.38%\n"
     ]
    }
   ],
   "source": [
    "model_rnn_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = model_3_batches(test_corpus_c_sww, batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)  # Add aspect_vectors\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)  # Add aspect_vectors\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_rnn_sww(sentence_vectors, aspect_vectors)  # Pass aspect_vectors\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fYZvJHOp22e"
   },
   "source": [
    "### Bi-GRU with attention by adding aspect with each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uGTuZlyppGv",
    "outputId": "a54370cd-c9fc-46ca-fae1-aad2005739a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 28.3723\n",
      "Epoch [2/20], Loss: 24.5645\n",
      "Epoch [3/20], Loss: 23.6989\n",
      "Epoch [4/20], Loss: 23.1126\n",
      "Epoch [5/20], Loss: 22.6465\n",
      "Epoch [6/20], Loss: 22.1926\n",
      "Epoch [7/20], Loss: 21.6821\n",
      "Epoch [8/20], Loss: 21.1328\n",
      "Epoch [9/20], Loss: 20.4914\n",
      "Epoch [10/20], Loss: 19.8538\n",
      "Epoch [11/20], Loss: 19.1883\n",
      "Epoch [12/20], Loss: 18.1199\n",
      "Epoch [13/20], Loss: 17.8897\n",
      "Epoch [14/20], Loss: 17.3439\n",
      "Epoch [15/20], Loss: 16.0932\n",
      "Epoch [16/20], Loss: 14.0347\n",
      "Epoch [17/20], Loss: 12.7591\n",
      "Epoch [18/20], Loss: 12.4764\n",
      "Epoch [19/20], Loss: 12.3456\n",
      "Epoch [20/20], Loss: 11.9433\n",
      "Train Accuracy: 80.94%\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim + input_size, 6, bias=False)\n",
    "        self.attn1 = nn.Linear(6 * max_len, max_len, bias=False)\n",
    "\n",
    "    def forward(self, output, aspect_vectors):\n",
    "        # lstm_output = [batch size, seq_len, hidden_dim]\n",
    "        aspect_vectors = aspect_vectors.unsqueeze(1).expand(-1, output.size(1), -1)\n",
    "        attention_scores = self.attn(torch.cat((output, aspect_vectors), dim=2))\n",
    "        # attention_scores = [batch size, seq_len, 6]\n",
    "        attention_scores = attention_scores.reshape(attention_scores.size(0), -1)\n",
    "        # attention_scores = [batch size, seq_len * 6]\n",
    "        #print(attention_scores.shape)\n",
    "        attention_scores1 = self.attn1(attention_scores)\n",
    "        # attention_scores1 = [batch size, 50]\n",
    "        return F.softmax(attention_scores1, dim=1)\n",
    "\n",
    "# Define the BiLSTM with Attention model\n",
    "class BiGRUClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional=True):\n",
    "        super(BiGRUClassifierWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.attention = Attention(hidden_size * 2 if bidirectional else hidden_size, input_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, aspect_vectors):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        gru_output, h_n = self.gru(x, h0)\n",
    "        attention_weights = self.attention(gru_output, aspect_vectors)\n",
    "        attention_weights = attention_weights.unsqueeze(2)\n",
    "        weighted = gru_output * attention_weights\n",
    "        weighted_sum = weighted.sum(dim=1)\n",
    "        out = self.fc(weighted_sum)\n",
    "        return out, attention_weights.squeeze(2)\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 50\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Batch Creation\n",
    "max_len = 70\n",
    "batches = model_3_batches(train_corpus_c_sww, batch_size, max_len)\n",
    "\n",
    "model_gru_sww = BiGRUClassifierWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_gru_sww.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_model = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs, attention_weights = model_gru_sww(sentence_vectors, aspect_vectors)\n",
    "        loss = criterion(outputs, polarities)\n",
    "        loss_model +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_model:.4f}')\n",
    "\n",
    "model_gru_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_gru_sww(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJkZWq2kpuZZ",
    "outputId": "68236fd4-b708-499a-cf64-14b5597b3153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.59%\n"
     ]
    }
   ],
   "source": [
    "model_gru_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = model_3_batches(test_corpus_c_sww, batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_gru_sww(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMFRmFlL9IIF"
   },
   "source": [
    "# 3.Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if3HpP3n-6Si"
   },
   "source": [
    "## Testing on Bi-LSTM varient-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qM_OIhF0WdH",
    "outputId": "5bfb0b67-ecf3-411d-9adb-55965c1d4130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 61.91%\n",
      "recall: 63.37%\n",
      "f1-score: 61.75%\n",
      "Test Accuracy: 63.37%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "model_1_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "  pred_polarity=[]\n",
    "  act_polarity=[]\n",
    "  correct=0\n",
    "  total=0\n",
    "  crr=0\n",
    "  batches = model_1_batches(test_corpus_c_sww, batch_size, max_len)\n",
    "  for batch in batches:\n",
    "    sentence_vectors, polarities = zip(*batch)\n",
    "    sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "    polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "    outputs = model_1_c_sww(sentence_vectors)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    pred_polarity.extend(predicted)\n",
    "    act_polarity.extend(polarities)\n",
    "    total += polarities.size(0)\n",
    "    correct += (predicted == polarities).sum().item()\n",
    "  accuracy = 100 * correct / total\n",
    "\n",
    "  act_polarity = [int(i) for i in act_polarity]\n",
    "  pred_polarity = [int(i) for i in pred_polarity]\n",
    "\n",
    "  p_score = precision_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "  r_score = recall_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "  f1_score = f1_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "\n",
    "  print(f'precision: {100*(p_score):.2f}%')\n",
    "  print(f'recall: {100*(r_score):.2f}%')\n",
    "  print(f'f1-score: {100*(f1_score):.2f}%')\n",
    "  print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRt0L4EL_Ujx"
   },
   "source": [
    "## Testing on Bi-LSTM varient-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVGoJcmoz8vm",
    "outputId": "a868f0a9-5222-4093-f843-345e313c3781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 62.74%\n",
      "recall: 63.60%\n",
      "f1-score: 62.58%\n",
      "Test Accuracy: 63.60%\n"
     ]
    }
   ],
   "source": [
    "model_2_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred_polarity=[]\n",
    "    act_polarity=[]\n",
    "    batches = model_2_batches(test_corpus_c_sww,batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, aspect_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        aspect_vectors = torch.stack(aspect_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "\n",
    "        outputs = model_2_c_sww(sentence_vectors, aspect_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred_polarity.extend(predicted)\n",
    "        act_polarity.extend(polarities)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "\n",
    "    act_polarity = [int(i) for i in act_polarity]\n",
    "    pred_polarity = [int(i) for i in pred_polarity]\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    p_score=precision_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "    r_score=recall_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "    f1_score=f1_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "\n",
    "    print(f'precision: {100*(p_score):.2f}%')\n",
    "    print(f'recall: {100*(r_score):.2f}%')\n",
    "    print(f'f1-score: {100*(f1_score):.2f}%')\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahKobuBz_mkv"
   },
   "source": [
    "## Testing on Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMeeI25M1JgY",
    "outputId": "e7551029-41ec-466c-8f0a-19d1967bd21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 64.33%\n",
      "recall: 64.04%\n",
      "f1-score: 64.14%\n",
      "Test Accuracy: 64.04%\n"
     ]
    }
   ],
   "source": [
    "model_c_sww.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred_polarity=[]\n",
    "    act_polarity=[]\n",
    "    batches = model_1_batches(test_corpus_c_sww,batch_size, max_len)\n",
    "    for batch in batches:\n",
    "        sentence_vectors, polarities = zip(*batch)\n",
    "        sentence_vectors = torch.stack(sentence_vectors).to(device)\n",
    "        polarities = torch.stack(polarities).to(device)\n",
    "        outputs, attention_weights = model_c_sww(sentence_vectors)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred_polarity.extend(predicted)\n",
    "        act_polarity.extend(polarities)\n",
    "        total += polarities.size(0)\n",
    "        correct += (predicted == polarities).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    act_polarity = [int(i) for i in act_polarity]\n",
    "    pred_polarity = [int(i) for i in pred_polarity]\n",
    "\n",
    "    p_score=precision_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "    r_score=recall_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "    f1_score=f1_score(act_polarity, pred_polarity, average=\"weighted\")\n",
    "\n",
    "    print(f'precision: {100*(p_score):.2f}%')\n",
    "    print(f'recall: {100*(r_score):.2f}%')\n",
    "    print(f'f1-score: {100*(f1_score):.2f}%')\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8D-cCmkmzhD",
    "outputId": "a0e1f352-dcc3-4416-e061-e6b59635105e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 28.2862\n",
      "Epoch [2/20], Loss: 24.5082\n",
      "Epoch [3/20], Loss: 23.7795\n",
      "Epoch [4/20], Loss: 23.1023\n",
      "Epoch [5/20], Loss: 22.5917\n",
      "Epoch [6/20], Loss: 22.2497\n",
      "Epoch [7/20], Loss: 21.7570\n",
      "Epoch [8/20], Loss: 21.0685\n",
      "Epoch [9/20], Loss: 20.4015\n",
      "Epoch [10/20], Loss: 19.8215\n",
      "Epoch [11/20], Loss: 18.9575\n",
      "Epoch [12/20], Loss: 18.0702\n",
      "Epoch [13/20], Loss: 17.6005\n",
      "Epoch [14/20], Loss: 17.2234\n",
      "Epoch [15/20], Loss: 16.0303\n",
      "Epoch [16/20], Loss: 14.5252\n",
      "Epoch [17/20], Loss: 14.0334\n",
      "Epoch [18/20], Loss: 12.5849\n",
      "Epoch [19/20], Loss: 11.5607\n",
      "Epoch [20/20], Loss: 11.7134\n",
      "Train Accuracy: 85.18%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1LzEtfJoTRt",
    "outputId": "4635a9fa-021c-46b1-c53d-2c2a967c6a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.82%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIl0Nf9fphI_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
